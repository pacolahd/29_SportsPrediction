# -*- coding: utf-8 -*-
"""29_SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17HtNh2aViaE1kbr--jilWtjwmjCc0-Dg
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Intro To AI/Project/players_21.csv')

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

df.head()

df.info(verbose =True, null_counts=True)

"""##Data Preprocessing
Demonstrated removing useless varaibles, EDA, imputation and encoding.

#### Prioritising on domain knowledge to remove some of the irrelevant data
"""

domain_removal=['player_url','short_name','long_name','sofifa_id','dob','age','height_cm','weight_kg','club_team_id','club_name','league_name','league_level','club_jersey_number',
                'club_loaned_from','club_joined','club_contract_valid_until','nationality_id','nationality_name','nation_team_id','nation_jersey_number','preferred_foot','weak_foot',
                'international_reputation','body_type','real_face','player_tags','player_traits','player_face_url','club_logo_url','club_flag_url','nation_logo_url','nation_flag_url']
df.drop(columns =domain_removal, inplace =True)
df.info(verbose =True, null_counts=True)

"""#### Most of the features with 3 letter abbreviations were not considered. Thus, we will convert to numeric form to recheck the correlation

Converting the abbreviated columns into numeric values to see if this numeric form would have an impact on their correlation values
"""

# Custom function to convert values to integers
def convert_to_integer(value):
      # Check if the value contains '+' or '-'
    if '+' in value:
        numbers = value.split('+')
        return int(numbers[0]) + int(numbers[1])
    elif '-' in value:
        numbers = value.split('-')
        return int(numbers[0]) - int(numbers[1])
    # If there's neither '+' nor '-', assume it's a plain number
    return int(value)

# List of columns you want to convert
columns_to_convert = [
    'ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam',
    'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm',
    'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk'
]

# Apply the custom function to each selected column
for column in columns_to_convert:
    df[column] = df[column].apply(convert_to_integer)

"""#### Based on Domain knowkedge, we are converting the some features into numeric to ckeck if they correlate with the overall data"""

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
df['nation_position'] = label_encoder.fit_transform(df['nation_position'])
df['club_positoin'] = label_encoder.fit_transform(df['club_position'])

df[['offensive_work_rate', 'defensive_work_rate']] = df['work_rate'].str.split('/', expand=True)

df['offensive_work_rate'] = label_encoder.fit_transform(df['offensive_work_rate'])
df['defensive_work_rate'] = label_encoder.fit_transform(df['defensive_work_rate'])

df.info(verbose =True, null_counts=True)

"""# Feature Engineering

Created feature subsets which show better correlation with the overall rating and scaled the independent variables

### Finding out the distinct player positions that are available
### Splitting the dataset into 4 distinct sets, such that different custom models can be applied to them
"""

# Create a set to store unique positions
unique_positions = set()

# Iterate through the list and split positions
p = df['player_positions']
for positions in p:
    positions_list = positions.split(', ')
    unique_positions.update(positions_list)

unique_positions

# Create copies for each category
# Define primary positions for each category
defender_positions = ['CB', 'LB', 'LWB', 'RB', 'RWB']
midfielder_positions = ['CAM', 'CDM', 'CM', 'LM', 'RM']
forward_positions = ['CF', 'LW', 'RW', 'ST']
goalkeeper_positions = ['GK']

# Function to assign primary position based on the new order
def assign_primary_position(row):
    positions = row['player_positions'].split(', ')

    if any(pos in positions for pos in goalkeeper_positions):
        return 'Goalkeeper'
    if any(pos in positions for pos in forward_positions):
        return 'Forward'
    if any(pos in positions for pos in midfielder_positions):
        return 'Midfielder'
    if any(pos in positions for pos in defender_positions):
        return 'Defender'

# Apply the function to create a new column 'primary_position'
df['primary_position'] = df.apply(assign_primary_position, axis=1)

# Filter the dataset to create non-redundant datasets

goalkeepers = df[df['primary_position'] == 'Goalkeeper'].copy()
forwards = df[df['primary_position'] == 'Forward'].copy()
midfielders = df[df['primary_position'] == 'Midfielder'].copy()
defenders = df[df['primary_position'] == 'Defender'].copy()

# Verifying if there was no redundancy in the datasets created, their contents should all sum to 18944
print(defenders.shape[0]+forwards.shape[0]+midfielders.shape[0]+goalkeepers.shape[0])

"""### Importing Relevant libraries"""

from sklearn.impute import SimpleImputer

"""### Performing a Correlation and Feature Extraction on the new goalkeeper-only dataset

"""

gk_corr = goalkeepers.corr()
gk_corr['overall'].sort_values(ascending=False)

temp = gk_corr[abs(gk_corr['overall']) > 0.75]['overall']
goalkeepers = goalkeepers[temp.index]
goalkeepers.info()

imputer=SimpleImputer(strategy='mean')
goalkeepers = pd.DataFrame(imputer.fit_transform(goalkeepers), columns=goalkeepers.columns)
goalkeepers.info()

"""### Performing a Correlation and Feature Extraction on the new defenders-only dataset"""

def_corr = defenders.corr()
def_corr['overall'].sort_values(ascending=False)

temp = def_corr[abs(def_corr['overall']) > 0.85]['overall']
defenders = defenders[temp.index]
redundant_columns_def = ['ldm','cdm','lcb','rcb','lb']
defenders = defenders.drop(columns = redundant_columns_def)

from sklearn.impute import SimpleImputer
imputer=SimpleImputer(strategy='mean')
defenders = pd.DataFrame(imputer.fit_transform(defenders), columns=defenders.columns)
defenders.info()

"""### Performing a Correlation and Feature Extraction on the new forwards-only dataset

"""

for_corr = forwards.corr()
for_corr['overall'].sort_values(ascending=False)

temp = for_corr[abs(for_corr['overall']) > 0.85]['overall']
forwards = forwards[temp.index]
redundant_columns_for = ['rf','lf','ram','lam','ls','rs','lw','lm','rcm','lcm']
forwards = forwards.drop(columns = redundant_columns_for)

imputer=SimpleImputer(strategy='mean')
forwards = pd.DataFrame(imputer.fit_transform(forwards), columns=forwards.columns)
forwards.info()

"""### Performing a Correlation and Feature Extraction on the new midfielders-only dataset

"""

mid_corr = midfielders.corr()
mid_corr['overall'].sort_values(ascending=False)

temp = mid_corr[abs(mid_corr['overall']) > 0.72]['overall']
midfielders = midfielders[temp.index]
redundant_columns_mid = ['rcm','lcm','lm','lam','ram','rs','ls','lf','rf','rw','lwb','rdm','ldm','rb','lb']
midfielders = midfielders.drop(columns = redundant_columns_mid)

imputer=SimpleImputer(strategy='mean')
midfielders = pd.DataFrame(imputer.fit_transform(midfielders), columns=midfielders.columns)
midfielders.info()

"""#Training The Models
Created and trained random_forest, xgboost, gradient_boosting, svm_regressor, voting_regressor, and stacking_regressor that can predict a player's overall rating

### Importing the Necessary Libraries
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import KFold, cross_val_score, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.kernel_ridge import KernelRidge

trained_models = {}

"""## 1. Training the goalkeeper subset of the data

### Splitting and Scaling the Data
"""

sc_gk = StandardScaler()
X_gk = goalkeepers.drop('overall',axis=1)
Y_gk = goalkeepers['overall']
scaled = sc_gk.fit_transform(X_gk)
X_gk = pd.DataFrame(scaled, columns=X_gk.columns)
Xtrain_gk, Xtest_gk, Ytrain_gk, Ytest_gk = train_test_split(X_gk, Y_gk, test_size=0.2, random_state=42)

"""### Training The Model

##### Performing cross validation with grid search for the 3 models together with the voting model, to determine and implement best hyperparameters
"""

# Defining Main cross-validation
cv = KFold(n_splits=5, shuffle=True, random_state=42)


# Initialize the individual models
random_forest = RandomForestRegressor(random_state=42)
xgboost = XGBRegressor(objective='reg:squarederror', random_state=42)
gradient_boosting = GradientBoostingRegressor(random_state=42) # , try init = base_estimator=DecisionTreeRegressor()
svm_regressor = SVR()


# Define hyperparameter grids for grid search
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.01, 0.1, 0.2]
}

param_grid_gb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.01, 0.1, 0.2]
}

param_grid_svm = {
    'C': [0.1, 1, 10],  # Regularization parameter
    'kernel': ['linear', 'rbf', 'sigmoid'],  # Kernel type
    'gamma': ['scale', 'auto', 0.1, 1, 10],  # Kernel coefficient
}

# Initialize grid search with cross-validation for individual models
grid_rf = GridSearchCV(estimator=random_forest, param_grid=param_grid_rf, scoring='neg_mean_squared_error', cv=cv)
grid_xgb = GridSearchCV(estimator=xgboost, param_grid=param_grid_xgb, scoring='neg_mean_squared_error', cv=cv)
grid_gb = GridSearchCV(estimator=gradient_boosting, param_grid=param_grid_gb, scoring='neg_mean_squared_error', cv=cv)
grid_svm = GridSearchCV(estimator=svm_regressor, param_grid=param_grid_svm, scoring='neg_mean_squared_error', cv=cv)

# Perform grid search to find the best hyperparameters for individual models
grid_rf.fit(Xtrain_gk,Ytrain_gk)
grid_xgb.fit(Xtrain_gk,Ytrain_gk)
grid_gb.fit(Xtrain_gk,Ytrain_gk)
grid_svm.fit(Xtrain_gk, Ytrain_gk)



# Get the best hyperparameters for each model
best_params_rf = grid_rf.best_params_
best_params_xgb = grid_xgb.best_params_
best_params_gb = grid_gb.best_params_
best_params_svm = grid_svm.best_params_


print("Random Forest Best Hyperparameters:", best_params_rf)
print("XGBoost Best Hyperparameters:", best_params_xgb)
print("Gradient Boosting Best Hyperparameters:", best_params_gb)
print("svm Best Hyperparameters:", best_params_svm)


# Initialize individual models with their best hyperparameters
random_forest.set_params(**best_params_rf)
xgboost.set_params(**best_params_xgb)
gradient_boosting.set_params(**best_params_gb)
svm_regressor.set_params(**best_params_svm)

# Goalkeepers Best Hyperparameters

#Random Forest Best Hyperparameters: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 300}
# XGBoost Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 4, 'n_estimators': 300}
# Gradient Boosting Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 300}
# Gradient Boosting Best Hyperparameters: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}

"""#### Performing cross validation with grid search for the voting model"""

# Initialize the Voting Regressor with best hyperparameters
voting_regressor = VotingRegressor(estimators=[
    ('rf', random_forest),
    ('xgb', xgboost),
    ('gb', gradient_boosting),
    ('svm', svm_regressor)
])

# Define hyperparameter grid for grid search and cross-validation of Voting Regressor
param_grid_voting = {
    'weights': [
        [1, 1, 1, 1],   # Equal weights
        [1, 2, 1, 1],   # Higher weight for the second model
        [2, 1, 1, 1],   # Higher weight for the first model
        [1, 1, 2, 1],   # Higher weight for the third model
        [1, 1, 1, 2],   # Higher weight for the fourth model
        [1, 1, 2, 2],   # Higher weight for the third and fourth models
    ]
}

# Initialize grid search with cross-validation for Voting Regressor
grid_voting = GridSearchCV(estimator=voting_regressor, param_grid=param_grid_voting, scoring='neg_mean_squared_error', cv=cv)

# Perform grid search to find the best hyperparameters for Voting Regressor
grid_voting.fit(Xtrain_gk,Ytrain_gk)

# Get the best hyperparameters for Voting Regressor
best_params_voting = grid_voting.best_params_

print("Voting Regressor Best Hyperparameters:", best_params_voting)

# Set the best hyperparameters for the Voting Regressor
voting_regressor.set_params(**best_params_voting)
# voting_regressor.set_params(weights=best_params_voting['weights'])

# Best Params after grid search
# Voting Regressor Best Hyperparameters: {'weights': [1, 1, 1, 2]}

"""#### Developing the stacking model (no cross validation yet)"""

# Initialize the Stacking Regressor
stacking_regressor = StackingRegressor(estimators=[
    ('rf', random_forest),
    ('xgb', xgboost),
    ('gb', gradient_boosting),
    ('svm', svm_regressor)
], final_estimator=LinearRegression())

"""#### Cross-Validation for Model Evaluation
This step helps us assess the model's overall performance and generalization capability. It's crucial to ensure that the model's performance isn't just an artifact of the particular subset of the data used during the initial grid search.

"""

# Calculate RMSE for all models
for model in  (random_forest, xgboost, gradient_boosting, svm_regressor, voting_regressor, stacking_regressor):
    scores = -cross_val_score(model, Xtrain_gk, Ytrain_gk, cv=cv, scoring='neg_mean_squared_error')
    rmse = np.sqrt(np.mean(scores))
    print(model.__class__.__name__, 'RMSE: ', rmse)

"""#### Final Evaluation on Testing Data:
Finally, we evaluate the model on the testing data (which the model has not seen during the training or hyperparameter tuning process)
"""

# Define the dataset name
dataset = 'goalkeepers'

# Create a dictionary to store the trained models with unique names


for model in (random_forest, xgboost, gradient_boosting, svm_regressor, voting_regressor, stacking_regressor):
    # Create a unique name for the model by combining the model's name and dataset name
    model_name = f"{model.__class__.__name__}_{dataset}"

    # Fit the model on the entire training dataset
    model.fit(Xtrain_gk, Ytrain_gk)

    # Make predictions on the testing data
    y_pred = model.predict(Xtest_gk)

    # Calculate RMSE and MAE on the testing data
    rmse = np.sqrt(mean_squared_error(y_pred, Ytest_gk))
    mae = mean_absolute_error(y_pred, Ytest_gk)

    print('\n', model_name, 'Final Evaluation RMSE on Testing Data: ', rmse)
    print(model_name, 'Final Evaluation MAE on Testing Data: ', mae)

    # Store the trained model in the dictionary with its unique name as the key
    trained_models[model_name] = model

"""#### Create a DataFrame with actual and predicted values to visualise the best model

"""

# SVR is proving to be the best here, even better than the stacking regressor.
gk_best_model_name = 'SVR_goalkeepers'
results_df = pd.DataFrame({'Actual': Ytest_gk, 'Predicted': np.round(trained_models[gk_best_model_name].predict(Xtest_gk))})

# Print the DataFrame
results_df

"""## 2. Training the defender subset of the data

### Splitting and Scaling the Data
"""

sc_def = StandardScaler()
X_def = defenders.drop('overall', axis=1)
Y_def = defenders['overall']
scaled = sc_def.fit_transform(X_def)
X_def = pd.DataFrame(scaled, columns=X_def.columns)
Xtrain_def, Xtest_def, Ytrain_def, Ytest_def = train_test_split(X_def, Y_def, test_size=0.2, random_state=42)

"""### Training The Model

##### Performing cross validation with grid search for the 3 models together with the voting model, to determine and implement best hyperparameters
"""

# Defining Main cross-validation
cv = KFold(n_splits=5, shuffle=True, random_state=42)


# Initialize the individual models
random_forest = RandomForestRegressor(random_state=42)
xgboost = XGBRegressor(objective='reg:squarederror', random_state=42)
gradient_boosting = GradientBoostingRegressor(random_state=42) # , try init = base_estimator=DecisionTreeRegressor()
svm_regressor = SVR()


# Define hyperparameter grids for grid search
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.01, 0.1, 0.2]
}

param_grid_gb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.01, 0.1, 0.2]
}

param_grid_svm = {
    'C': [0.1, 1, 10],  # Regularization parameter
    'kernel': ['linear', 'rbf', 'sigmoid'],  # Kernel type
    'gamma': ['scale', 'auto', 0.1, 1, 10],  # Kernel coefficient
}

# Initialize grid search with cross-validation for individual models
grid_rf = GridSearchCV(estimator=random_forest, param_grid=param_grid_rf, scoring='neg_mean_squared_error', cv=cv)
grid_xgb = GridSearchCV(estimator=xgboost, param_grid=param_grid_xgb, scoring='neg_mean_squared_error', cv=cv)
grid_gb = GridSearchCV(estimator=gradient_boosting, param_grid=param_grid_gb, scoring='neg_mean_squared_error', cv=cv)
grid_svm = GridSearchCV(estimator=svm_regressor, param_grid=param_grid_svm, scoring='neg_mean_squared_error', cv=cv)

# Perform grid search to find the best hyperparameters for individual models
grid_rf.fit(Xtrain_def,Ytrain_def)
grid_xgb.fit(Xtrain_def,Ytrain_def)
grid_gb.fit(Xtrain_def,Ytrain_def)
grid_svm.fit(Xtrain_def, Ytrain_def)



# Get the best hyperparameters for each model
best_params_rf = grid_rf.best_params_
best_params_xgb = grid_xgb.best_params_
best_params_gb = grid_gb.best_params_
best_params_svm = grid_svm.best_params_


print("Random Forest Best Hyperparameters:", best_params_rf)
print("XGBoost Best Hyperparameters:", best_params_xgb)
print("Gradient Boosting Best Hyperparameters:", best_params_gb)
print("svm Best Hyperparameters:", best_params_svm)


# Initialize individual models with their best hyperparameters
random_forest.set_params(**best_params_rf)
xgboost.set_params(**best_params_xgb)
gradient_boosting.set_params(**best_params_gb)
svm_regressor.set_params(**best_params_svm)

# Defenders Best Hyperparameters
# Random Forest Best Hyperparameters: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 300}
# XGBoost Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300}
# Gradient Boosting Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300}
# Gradient Boosting Best Hyperparameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}

"""#### Performing cross validation with grid search for the voting model"""

from re import search
# Initialize the Voting Regressor with best hyperparameters
voting_regressor = VotingRegressor(estimators=[
    ('rf', random_forest),
    ('xgb', xgboost),
    ('gb', gradient_boosting),
    ('svm', svm_regressor)
])

# Define hyperparameter grid for grid search and cross-validation of Voting Regressor
param_grid_voting = {
    'weights': [
        [1, 1, 1, 1],   # Equal weights
        [1, 2, 1, 1],   # Higher weight for the second model
        [2, 1, 1, 1],   # Higher weight for the first model
        [1, 1, 2, 1],   # Higher weight for the third model
        [1, 1, 1, 2],   # Higher weight for the fourth model
        [1, 1, 2, 2],   # Higher weight for the third and fourth models
    ]
}

# Initialize grid search with cross-validation for Voting Regressor
grid_voting = GridSearchCV(estimator=voting_regressor, param_grid=param_grid_voting, scoring='neg_mean_squared_error', cv=cv)

# Perform grid search to find the best hyperparameters for Voting Regressor
grid_voting.fit(Xtrain_def,Ytrain_def)

# Get the best hyperparameters for Voting Regressor
best_params_voting = grid_voting.best_params_

print("Voting Regressor Best Hyperparameters:", best_params_voting)

# Set the best hyperparameters for the Voting Regressor
voting_regressor.set_params(**best_params_voting)
# voting_regressor.set_params(weights=best_params_voting['weights'])

# Best Params after grid search
# Voting Regressor Best Hyperparameters: {'weights': [2, 1, 1, 1]}

"""#### Developing the stacking model"""

# Initialize the Stacking Regressor
stacking_regressor = StackingRegressor(estimators=[
    ('rf', random_forest),
    ('xgb', xgboost),
    ('gb', gradient_boosting),
    ('svm', svm_regressor)
], final_estimator=LinearRegression())

"""#### Cross-Validation for Model Evaluation
This step helps us assess the model's overall performance and generalization capability. It's crucial to ensure that the model's performance isn't just an artifact of the particular subset of the data used during the initial grid search.

"""

# Calculate RMSE for all models
for model in  (random_forest, xgboost, gradient_boosting, svm_regressor, voting_regressor, stacking_regressor):
    scores = -cross_val_score(model, Xtrain_def, Ytrain_def, cv=cv, scoring='neg_mean_squared_error')
    rmse = np.sqrt(np.mean(scores))
    print(model.__class__.__name__, 'RMSE: ', rmse)

"""#### Final Evaluation on Testing Data:
Finally, we evaluate the model on the testing data (which the model has not seen during the training or hyperparameter tuning process)
"""

# Define the dataset name
dataset = 'defenders'


for model in (random_forest, xgboost, gradient_boosting, svm_regressor, voting_regressor, stacking_regressor):
    # Create a unique name for the model by combining the model's name and dataset name
    model_name = f"{model.__class__.__name__}_{dataset}"

    # Fit the model on the entire training dataset
    model.fit(Xtrain_def, Ytrain_def)

    # Make predictions on the testing data
    y_pred = model.predict(Xtest_def)

    # Calculate RMSE and MAE on the testing data
    rmse = np.sqrt(mean_squared_error(y_pred, Ytest_def))
    mae = mean_absolute_error(y_pred, Ytest_def)

    print('\n', model_name, 'Final Evaluation RMSE on Testing Data: ', rmse)
    print(model_name, 'Final Evaluation MAE on Testing Data: ', mae)

    # Store the trained model in the dictionary with its unique name as the key
    trained_models[model_name] = model

"""#### Create a DataFrame with actual and predicted values to visualise the best model

"""

# SVR is proving to be the best here, even better than the stacking regressor.
def_best_model_name = 'StackingRegressor_defenders'
results_df = pd.DataFrame({'Actual': Ytest_def, 'Predicted': np.round(trained_models[def_best_model_name].predict(Xtest_def))})

# Print the DataFrame
results_df



"""## 3. Training the forwards subset of the data

### Splitting and Scaling the Data
"""

sc_for = StandardScaler()
X_for = forwards.drop('overall', axis=1)
Y_for = forwards['overall']
scaled_for = sc_for.fit_transform(X_for)
X_for = pd.DataFrame(scaled_for, columns=X_for.columns)
Xtrain_for, Xtest_for, Ytrain_for, Ytest_for = train_test_split(X_for, Y_for, test_size=0.2, random_state=42)

"""### Training The Model

##### Performing cross validation with grid search for the 3 models together with the voting model, to determine and implement best hyperparameters
"""

# Defining Main cross-validation
cv = KFold(n_splits=5, shuffle=True, random_state=42)


# Initialize the individual models
random_forest = RandomForestRegressor(random_state=42)
xgboost = XGBRegressor(objective='reg:squarederror', random_state=42)
# gradient_boosting = GradientBoostingRegressor(random_state=42) # , try init = base_estimator=DecisionTreeRegressor()
svm_regressor = SVR()


# Define hyperparameter grids for grid search
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.01, 0.1, 0.2]
}

# param_grid_gb = {
#     'n_estimators': [100, 200, 300],
#     'max_depth': [3, 4, 5],
#     'learning_rate': [0.01, 0.1, 0.2]
# }

param_grid_svm = {
    'C': [0.1, 1, 10],  # Regularization parameter
    'kernel': ['linear', 'rbf', 'sigmoid'],  # Kernel type
    'gamma': ['scale', 'auto', 0.1, 1, 10],  # Kernel coefficient
}

# Initialize grid search with cross-validation for individual models
grid_rf = GridSearchCV(estimator=random_forest, param_grid=param_grid_rf, scoring='neg_mean_squared_error', cv=cv)
grid_xgb = GridSearchCV(estimator=xgboost, param_grid=param_grid_xgb, scoring='neg_mean_squared_error', cv=cv)
# grid_gb = GridSearchCV(estimator=gradient_boosting, param_grid=param_grid_gb, scoring='neg_mean_squared_error', cv=cv)
grid_svm = GridSearchCV(estimator=svm_regressor, param_grid=param_grid_svm, scoring='neg_mean_squared_error', cv=cv)

# Perform grid search to find the best hyperparameters for individual models
grid_rf.fit(Xtrain_for,Ytrain_for)
grid_xgb.fit(Xtrain_for,Ytrain_for)
# grid_gb.fit(Xtrain_for,Ytrain_for)
grid_svm.fit(Xtrain_for, Ytrain_for)



# Get the best hyperparameters for each model
best_params_rf = grid_rf.best_params_
best_params_xgb = grid_xgb.best_params_
# best_params_gb = grid_gb.best_params_
best_params_svm = grid_svm.best_params_


print("Random Forest Best Hyperparameters:", best_params_rf)
print("XGBoost Best Hyperparameters:", best_params_xgb)
# print("Gradient Boosting Best Hyperparameters:", best_params_gb)
print("svm Boosting Best Hyperparameters:", best_params_svm)


# Initialize individual models with their best hyperparameters
random_forest.set_params(**best_params_rf)
xgboost.set_params(**best_params_xgb)
# gradient_boosting.set_params(**best_params_gb)
svm_regressor.set_params(**best_params_svm)


# Forward Best HyperParameters
# Random Forest Best Hyperparameters: {'max_depth': 30, 'min_samples_split': 2, 'n_estimators': 300}
# XGBoost Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}
# svm Best Hyperparameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}

"""#### Performing cross validation with grid search for the voting model"""

# Initialize the Voting Regressor with best hyperparameters
voting_regressor = VotingRegressor(estimators=[
    ('rf', random_forest),
    ('xgb', xgboost),
    # ('gb', gradient_boosting),
    ('svm', svm_regressor)
])

# Define hyperparameter grid for grid search and cross-validation of Voting Regressor
param_grid_voting = {
    'weights': [
        [1, 1, 1],   # Equal weights
        [2, 1, 1],
        [1, 2, 1],
        [1, 1, 2]

    ]
}

# Initialize grid search with cross-validation for Voting Regressor
grid_voting = GridSearchCV(estimator=voting_regressor, param_grid=param_grid_voting, scoring='neg_mean_squared_error', cv=cv)

# Perform grid search to find the best hyperparameters for Voting Regressor
grid_voting.fit(Xtrain_for,Ytrain_for)

# Get the best hyperparameters for Voting Regressor
best_params_voting = grid_voting.best_params_

print("Voting Regressor Best Hyperparameters:", best_params_voting)

# Set the best hyperparameters for the Voting Regressor
voting_regressor.set_params(**best_params_voting)
# voting_regressor.set_params(weights=best_params_voting['weights'])

# Best Params after grid search
#Voting Regressor Best Hyperparameters: {'weights': [1, 1, 2]}

"""#### Developing the stacking model"""

# Initialize the Stacking Regressor
stacking_regressor = StackingRegressor(estimators=[
    ('rf', random_forest),
    ('xgb', xgboost),
    ('svm', svm_regressor)
], final_estimator=LinearRegression())

"""#### Cross-Validation for Model Evaluation
This step helps us assess the model's overall performance and generalization capability. It's crucial to ensure that the model's performance isn't just an artifact of the particular subset of the data used during the initial grid search.

"""

# Calculate RMSE for all models
for model in  (random_forest, xgboost, svm_regressor, voting_regressor, stacking_regressor):
    scores = -cross_val_score(model, Xtrain_for, Ytrain_for, cv=cv, scoring='neg_mean_squared_error')
    rmse = np.sqrt(np.mean(scores))
    print(model.__class__.__name__, 'RMSE: ', rmse)

"""#### Final Evaluation on Testing Data:
Finally, we evaluate the model on the testing data (which the model has not seen during the training or hyperparameter tuning process)
"""

# Define the dataset name
dataset = 'forwards'

# Create a dictionary to store the trained models with unique names


for model in (random_forest, xgboost, svm_regressor, voting_regressor, stacking_regressor):
    # Create a unique name for the model by combining the model's name and dataset name
    model_name = f"{model.__class__.__name__}_{dataset}"

    # Fit the model on the entire training dataset
    model.fit(Xtrain_for, Ytrain_for)

    # Make predictions on the testing data
    y_pred = model.predict(Xtest_for)

    # Calculate RMSE and MAE on the testing data
    rmse = np.sqrt(mean_squared_error(y_pred, Ytest_for))
    mae = mean_absolute_error(y_pred, Ytest_for)

    print('\n', model_name, 'Final Evaluation RMSE on Testing Data: ', rmse)
    print(model_name, 'Final Evaluation MAE on Testing Data: ', mae)

    # Store the trained model in the dictionary with its unique name as the key
    trained_models[model_name] = model

"""#### Create a DataFrame with actual and predicted values to visualise the best model

"""

# SVR is proving to be the best here, even better than the stacking regressor.
for_best_model_name = 'StackingRegressor_forwards'
results_df = pd.DataFrame({'Actual': Ytest_for, 'Predicted': np.round(trained_models[for_best_model_name].predict(Xtest_for))})

# Print the DataFrame
results_df

"""## 4. Training the midfielders subset of the data

### Splitting and Scaling the Data
"""

sc_mid = StandardScaler()
X_mid = midfielders.drop('overall', axis=1)
Y_mid = midfielders['overall']
scaled_mid = sc_mid.fit_transform(X_mid)
X_mid = pd.DataFrame(scaled_mid, columns=X_mid.columns)
Xtrain_mid, Xtest_mid, Ytrain_mid, Ytest_mid = train_test_split(X_mid, Y_mid, test_size=0.2, random_state=42)

"""### Training The Model

##### Performing cross validation with grid search for the 3 models together with the voting model, to determine and implement best hyperparameters
"""

# Defining Main cross-validation
cv = KFold(n_splits=5, shuffle=True, random_state=42)


# Initialize the individual models
random_forest = RandomForestRegressor(random_state=42)
xgboost = XGBRegressor(objective='reg:squarederror', random_state=42)
# gradient_boosting = GradientBoostingRegressor(random_state=42) # , try init = base_estimator=DecisionTreeRegressor()
svm_regressor = SVR()
kernel_ridge = KernelRidge()



# Define hyperparameter grids for grid search
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10]
}

param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.01, 0.1, 0.2]
}

# param_grid_gb = {
#     'n_estimators': [100, 200, 300],
#     'max_depth': [3, 4, 5],
#     'learning_rate': [0.01, 0.1, 0.2]
# }

param_grid_svm = {
    'C': [0.1, 1, 10],  # Regularization parameter
    'kernel': ['linear', 'rbf', 'sigmoid'],  # Kernel type
    'gamma': ['scale', 'auto', 0.1, 1, 10],  # Kernel coefficient
}


param_grid_kernel_ridge = {
    'alpha': [0.1, 1, 10],  # Regularization parameter
    'kernel': ['linear', 'polynomial', 'rbf'],  # Kernel type
    'gamma': [None, 0.1, 1, 10],  # Kernel coefficient
    'degree': [2, 3, 4],  # Degree of the polynomial kernel
}



# Initialize grid search with cross-validation for individual models
grid_rf = GridSearchCV(estimator=random_forest, param_grid=param_grid_rf, scoring='neg_mean_squared_error', cv=cv)
grid_xgb = GridSearchCV(estimator=xgboost, param_grid=param_grid_xgb, scoring='neg_mean_squared_error', cv=cv)
# grid_gb = GridSearchCV(estimator=gradient_boosting, param_grid=param_grid_gb, scoring='neg_mean_squared_error', cv=cv)
grid_svm = GridSearchCV(estimator=svm_regressor, param_grid=param_grid_svm, scoring='neg_mean_squared_error', cv=cv)
grid_kernel_ridge = GridSearchCV(estimator=kernel_ridge, param_grid=param_grid_kernel_ridge, scoring='neg_mean_squared_error', cv=cv)


# Perform grid search to find the best hyperparameters for individual models
grid_rf.fit(Xtrain_mid,Ytrain_mid)
grid_xgb.fit(Xtrain_mid,Ytrain_mid)
# grid_gb.fit(Xtrain_mid,Ytrain_mid)
grid_svm.fit(Xtrain_mid, Ytrain_mid)
grid_kernel_ridge.fit(Xtrain_mid, Ytrain_mid)



# Get the best hyperparameters for each model
best_params_rf = grid_rf.best_params_
best_params_xgb = grid_xgb.best_params_
# best_params_gb = grid_gb.best_params_
best_params_svm = grid_svm.best_params_
best_params_kernel_ridge = grid_kernel_ridge.best_params_


print("Random Forest Best Hyperparameters:", best_params_rf)
print("XGBoost Best Hyperparameters:", best_params_xgb)
# print("Gradient Boosting Best Hyperparameters:", best_params_gb)
print("svm Best Hyperparameters:", best_params_svm)
print("Kernel Ridge Best Hyperparameters:", best_params_kernel_ridge)



# Initialize individual models with their best hyperparameters
random_forest.set_params(**best_params_rf)
xgboost.set_params(**best_params_xgb)
gradient_boosting.set_params(**best_params_gb)
svm_regressor.set_params(**best_params_svm)
kernel_ridge.set_params(**best_params_kernel_ridge)


# Midfielder Best Hyperparameters
# Random Forest Best Hyperparameters: {'max_depth': 30, 'min_samples_split': 2, 'n_estimators': 300}
# XGBoost Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300}
# svm Best Hyperparameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}
# Kernel Ridge Best Hyperparameters: {'alpha': 1, 'degree': 4, 'gamma': None, 'kernel': 'polynomial'}

"""#### Performing cross validation with grid search for the voting model"""

# Initialize the Voting Regressor with best hyperparameters
voting_regressor = VotingRegressor(estimators=[
    ('rf', random_forest),
    ('xgb', xgboost),
    ('svm', svm_regressor),
    ('kr', kernel_ridge)
])

# Define hyperparameter grid for grid search and cross-validation of Voting Regressor
param_grid_voting = {
    'weights': [
        [1, 1, 1, 1],   # Equal weights
        [1, 2, 1, 1],   # Higher weight for the second model
        [2, 1, 1, 1],   # Higher weight for the first model
        [1, 1, 2, 1],   # Higher weight for the third model
        [1, 1, 1, 2],   # Higher weight for the fourth model
        [1, 1, 2, 2],   # Higher weight for the third and fourth models
    ]
}

# Initialize grid search with cross-validation for Voting Regressor
grid_voting = GridSearchCV(estimator=voting_regressor, param_grid=param_grid_voting, scoring='neg_mean_squared_error', cv=cv)

# Perform grid search to find the best hyperparameters for Voting Regressor
grid_voting.fit(Xtrain_mid,Ytrain_mid)

# Get the best hyperparameters for Voting Regressor
best_params_voting = grid_voting.best_params_

print("Voting Regressor Best Hyperparameters:", best_params_voting)

# Set the best hyperparameters for the Voting Regressor
voting_regressor.set_params(**best_params_voting)
# voting_regressor.set_params(weights=best_params_voting['weights'])

# Best Params after grid search:
#Voting Regressor Best Hyperparameters: {'weights': [1, 1, 1, 2]}

"""#### Developing the stacking model"""

# Initialize the Stacking Regressor
stacking_regressor = StackingRegressor(estimators=[
    ('rf', random_forest),
    ('xgb', xgboost),
    ('svm', svm_regressor),
    ('kr', kernel_ridge)
], final_estimator=LinearRegression())

"""#### Cross-Validation for Model Evaluation
This step helps us assess the model's overall performance and generalization capability. It's crucial to ensure that the model's performance isn't just an artifact of the particular subset of the data used during the initial grid search.

"""

# Calculate RMSE for all models
for model in  (random_forest, xgboost, svm_regressor, kernel_ridge, voting_regressor, stacking_regressor):
    scores = -cross_val_score(model, Xtrain_mid, Ytrain_mid, cv=cv, scoring='neg_mean_squared_error')
    rmse = np.sqrt(np.mean(scores))
    print(model.__class__.__name__, 'RMSE: ', rmse)

"""#### Final Evaluation on Testing Data:
Finally, we evaluate the model on the testing data (which the model has not seen during the training or hyperparameter tuning process)
"""

for model in trained_models:
  print(model)



# Define the dataset name
dataset = 'midfielders'

# Create a dictionary to store the trained models with unique names


for model in (random_forest, xgboost, svm_regressor, kernel_ridge, voting_regressor, stacking_regressor):
    # Create a unique name for the model by combining the model's name and dataset name
    model_name = f"{model.__class__.__name__}_{dataset}"

    # Fit the model on the entire training dataset
    model.fit(Xtrain_mid, Ytrain_mid)

    # Make predictions on the testing data
    y_pred = model.predict(Xtest_mid)

    # Calculate RMSE and MAE on the testing data
    rmse = np.sqrt(mean_squared_error(y_pred, Ytest_mid))
    mae = mean_absolute_error(y_pred, Ytest_mid)

    print('\n', model_name, 'Final Evaluation RMSE on Testing Data: ', rmse)
    print(model_name, 'Final Evaluation MAE on Testing Data: ', mae)

    # Store the trained model in the dictionary with its unique name as the key
    trained_models[model_name] = model

"""#### Create a DataFrame with actual and predicted values to visualise the best model

"""

# SVR is proving to be the best here, even better than the stacking regressor.
mid_best_model_name = 'StackingRegressor_midfielders'
results_df = pd.DataFrame({'Actual': Ytest_mid, 'Predicted': np.round(trained_models[mid_best_model_name].predict(Xtest_mid))})

# Print the DataFrame
results_df

"""# 2022 dataset

Oganising data to look like 2021 format

###This criterion is linked to a learning outcomeTest with new data set
Used the data(players_22) to test how good is the model with completely new data.
"""

import pandas as pd
df2=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Intro To AI/Project/players_22.csv')

df2.head()

df2.info(verbose =True, null_counts=True)

"""## Data Preprocessing

### Converting the abbreviated columns into numeric values to see if this numeric form would have an impact on their correlation values
"""

# Custom function to convert values to integers
def convert_to_integer(value):
      # Check if the value contains '+' or '-'
    if '+' in value:
        numbers = value.split('+')
        return int(numbers[0]) + int(numbers[1])
    elif '-' in value:
        numbers = value.split('-')
        return int(numbers[0]) - int(numbers[1])
    # If there's neither '+' nor '-', assume it's a plain number
    return int(value)

# List of columns you want to convert
columns_to_convert = [
    'ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam',
    'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm',
    'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk'
]

# Apply the custom function to each selected column
for column in columns_to_convert:
    df2[column] = df2[column].apply(convert_to_integer)

"""### Splitting the dataset into 4 distinct sets, such that different custom models can be applied to them"""

# Create copies for each category
# Define primary positions for each category
defender_positions = ['CB', 'LB', 'LWB', 'RB', 'RWB']
midfielder_positions = ['CAM', 'CDM', 'CM', 'LM', 'RM']
forward_positions = ['CF', 'LW', 'RW', 'ST']
goalkeeper_positions = ['GK']

# Function to assign primary position based on the new order
def assign_primary_position(row):
    positions = row['player_positions'].split(', ')

    if any(pos in positions for pos in goalkeeper_positions):
        return 'Goalkeeper'
    if any(pos in positions for pos in forward_positions):
        return 'Forward'
    if any(pos in positions for pos in midfielder_positions):
        return 'Midfielder'
    if any(pos in positions for pos in defender_positions):
        return 'Defender'

# Apply the function to create a new column 'primary_position'
df2['primary_position'] = df2.apply(assign_primary_position, axis=1)

# Filter the dataset to create non-redundant datasets

goalkeepers2 = df2[df2['primary_position'] == 'Goalkeeper'].copy()
forwards2 = df2[df2['primary_position'] == 'Forward'].copy()
midfielders2 = df2[df2['primary_position'] == 'Midfielder'].copy()
defenders2 = df2[df2['primary_position'] == 'Defender'].copy()

# Verifying if there was no redundancy in the datasets created, their contents should all sum to 18944
print(defenders2.shape[0]+forwards2.shape[0]+midfielders2.shape[0]+goalkeepers2.shape[0])

"""## Testing

### Testing the goalkeeper model with the 2022 data
"""

# Import necessary libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()

goalkeepers2 = goalkeepers2[goalkeepers.columns.tolist()]
from sklearn.impute import SimpleImputer
imputer=SimpleImputer(strategy='mean')
golkeepers2 = pd.DataFrame(imputer.fit_transform(goalkeepers2), columns=goalkeepers2.columns)
golkeepers2.info()

X_gk2=goalkeepers2.drop('overall',axis=1)
Y_gk2=goalkeepers2['overall']
scaled = sc_gk.fit_transform(X_gk2)
X_gk2 = pd.DataFrame(scaled, columns=X_gk2.columns)

# Make predictions on the testing data
y_pred = trained_models[gk_best_model_name].predict(X_gk2)
# Calculate RMSE and MAE on the testing data
rmse = np.sqrt(mean_squared_error(y_pred, Y_gk2))
mae = mean_absolute_error(y_pred, Y_gk2)

print('\n', gk_best_model_name, 'Final Evaluation RMSE on Testing Data: ', rmse)
print(gk_best_model_name, 'Final Evaluation MAE on Testing Data: ', mae)

results_df = pd.DataFrame({'Actual': Y_gk2, 'Predicted': trained_models[gk_best_model_name].predict(X_gk2)})

# Print the DataFrame
results_df

"""### Testing the defender model with the 2022 data"""

defenders2 = defenders2[defenders.columns.tolist()]
from sklearn.impute import SimpleImputer
imputer=SimpleImputer(strategy='mean')
defenders2 = pd.DataFrame(imputer.fit_transform(defenders2), columns=defenders2.columns)
defenders2.info()

X_def2 = defenders2.drop('overall',axis=1)
Y_def2=defenders2['overall']
scaled = sc_def.fit_transform(X_def2)
X_def2 = pd.DataFrame(scaled, columns=X_def2.columns)

# Make predictions on the testing data
y_pred = trained_models[def_best_model_name].predict(X_def2)
# Calculate RMSE and MAE on the testing data
rmse = np.sqrt(mean_squared_error(y_pred, Y_def2))
mae = mean_absolute_error(y_pred, Y_def2)

print('\n', def_best_model_name, 'Final Evaluation RMSE on Testing Data: ', rmse)
print(def_best_model_name, 'Final Evaluation MAE on Testing Data: ', mae)

results_df = pd.DataFrame({'Actual': Y_def2, 'Predicted': np.round(y_pred)})

# Print the DataFrame
results_df

"""### Testing the forwards model with the 2022 data"""

forwards2 = forwards2[forwards.columns.tolist()]
from sklearn.impute import SimpleImputer
imputer=SimpleImputer(strategy='mean')
forwards2 = pd.DataFrame(imputer.fit_transform(forwards2), columns=forwards2.columns)

X_for2=forwards2.drop('overall',axis=1)
Y_for2=forwards2['overall']
scaled = sc_for.fit_transform(X_for2)
X_for2 = pd.DataFrame(scaled, columns = X_for2.columns)

# Make predictions on the testing data
y_pred = trained_models[for_best_model_name].predict(X_for2)
# Calculate RMSE and MAE on the testing data
rmse = np.sqrt(mean_squared_error(y_pred, Y_for2))
mae = mean_absolute_error(y_pred, Y_for2)

print('\n', for_best_model_name, 'Final Evaluation RMSE on Testing Data: ', rmse)
print(for_best_model_name, 'Final Evaluation MAE on Testing Data: ', mae)

results_df = pd.DataFrame({'Actual': Y_for2, 'Predicted': np.round(y_pred)})

# Print the DataFrame
results_df

"""### Testing the midfielder model with the 2022 data"""

midfielders2 = midfielders2[midfielders.columns.tolist()]
from sklearn.impute import SimpleImputer
imputer=SimpleImputer(strategy='mean')
midfielders2 = pd.DataFrame(imputer.fit_transform(midfielders2), columns=midfielders2.columns)
midfielders2.info()

X_mid2=midfielders2.drop('overall',axis=1)
Y_mid2=midfielders2['overall']
scaled = sc_mid.fit_transform(X_mid2)
X_mid2 = pd.DataFrame(scaled, columns=X_mid2.columns)

# Make predictions on the testing data
y_pred = trained_models[mid_best_model_name].predict(X_mid2)
# Calculate RMSE and MAE on the testing data
rmse = np.sqrt(mean_squared_error(y_pred, Y_mid2))
mae = mean_absolute_error(y_pred, Y_mid2)

print('\n', mid_best_model_name, 'Final Evaluation RMSE on Testing Data: ', rmse)
print(mid_best_model_name, 'Final Evaluation MAE on Testing Data: ', mae)

results_df = pd.DataFrame({'Actual': Y_mid2, 'Predicted': np.round(y_pred)})

# Print the DataFrame
results_df

"""# Deployment

Deployed the model on a simple web page using either (Heroku, Streamlite or Flask) and shared a link to the video that shows how the model performs on the web page/site

### Exporting the Models and their respective standard scalers, since the data has to be fitted and scaled before being fed to the model

#### A Birds Eye view of the trained models
"""

c = 0
for model in trained_models:
  print(model)
  c+=1
print(c, 'Models in total')

"""#### Exporting Models and Scalers

The Scalers are important because the input must be scaled the same way they were before the models were trained
"""

import pickle

# Save the models to a file
with open('forward_model.pkl', 'wb') as file:
    pickle.dump(trained_models[for_best_model_name], file)

with open('defenders_model.pkl', 'wb') as file:
    pickle.dump(trained_models[def_best_model_name], file)

with open('midfielders_model.pkl', 'wb') as file:
    pickle.dump(trained_models[mid_best_model_name], file)

with open('goalkeepers_model.pkl', 'wb') as file:
    pickle.dump(trained_models[gk_best_model_name], file)



# Save the scalers to a file
with open('forward_scaler.pkl', 'wb') as file:
    pickle.dump(sc_for, file)

with open('defenders_scaler.pkl', 'wb') as file:
    pickle.dump(sc_def, file)

with open('midfielders_scaler.pkl', 'wb') as file:
    pickle.dump(sc_mid, file)

with open('goalkeepers_scaler.pkl', 'wb') as file:
    pickle.dump(sc_gk, file)

"""Deployment was done on streamlit"""

